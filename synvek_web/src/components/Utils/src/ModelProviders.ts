export interface ModelFile {
  repoName: string
  repoFile: string
}

export interface ModelRepo {
  repoName: string
}

export interface ModelOption {
  name: string
  fileSize: string
  repos: ModelRepo[]
  files: ModelFile[]
}

export type ModelCategory =
  | 'text-to-text'
  | 'image-to-text'
  | 'speech-to-text'
  | 'video-to-text'
  | 'text-to-image'
  | 'text-to-speech'
  | 'text-to-video'
  | 'image-to-video'
export type ModelType = 'run' | 'plain' | 'vision-plain' | 'diffusion' | 'speech' | 'uqff' | 'gguf' | 'stable-diffusion'
export type BackendType = 'default' | 'llama_cpp' | 'stable_diffusion_cpp' | 'whisper_cpp'
export type AccelerationType = 'cpu' | 'cuda' | 'cuda_legacy' | 'metal' | 'cpu-mkl' | 'cpu-accelerate' | 'vulkan' | 'opencl' | 'webgpu' | 'hip'

export interface ModelProviderStatus {
  name: string
  modelId: string
  modelCreator?: string
  modelSource?: string
  downloading?: boolean
  downloaded?: boolean
  accessToken?: string
  mirror?: string
  repos?: ModelRepo[]
  files?: ModelFile[]
}

export interface ModelProvider {
  modelId: string
  modelCreator: string
  modelSource: 'huggingface' | 'modelscope'
  modelOptions: ModelOption[]
  categories: ModelCategory[]
  backends: BackendType[]
  modelType: ModelType
  supportISQ: boolean
  isAnyMoE: boolean
  supportTool: boolean
  supportOffloaded: boolean
  supportThinking: boolean
  supportMoQE: boolean
  adapter?: 'x-lora' | 'lora'
  chatTemplate?: string
  summary: string
  description: string
  accessTokenRequired: boolean
  extraArgs: string[]
  //Model may have optimized steps count as default steps count
  defaultStepsCount?: number
  //Model may have optimized cfg scale as default
  defaultCfgScale?: number
  supportStepsCount?: boolean
  supportCfgScale?: boolean
  supportNegativePrompt?: boolean
  supportDiffusionFA?: boolean
  supportOffloadedToCPU?: boolean
  supportImageEdit?: boolean
  supportVideoGen?: boolean
}

export const modelProviders: ModelProvider[] = [
  {
    modelId: 'GPT-OSS-20b-GGUF',
    modelCreator: 'ggml-org',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'ggml-org/gpt-oss-20b-GGUF',
        fileSize: '11.2GB',
        repos: [],
        files: [{ repoName: 'ggml-org/gpt-oss-20b-GGUF', repoFile: 'gpt-oss-20b-mxfp4.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'GPT OSS 20b GGUF',
    description: 'GPT OSS 20b GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'GPT-OSS-120b-GGUF',
    modelCreator: 'ggml-org',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'ggml-org/gpt-oss-120b-GGUF',
        fileSize: '59GB',
        repos: [],
        files: [
          { repoName: 'ggml-org/gpt-oss-120b-GGUF', repoFile: 'gpt-oss-120b-mxfp4-00001-of-00003.gguf' },
          { repoName: 'ggml-org/gpt-oss-120b-GGUF', repoFile: 'gpt-oss-120b-mxfp4-00002-of-00003.gguf' },
          { repoName: 'ggml-org/gpt-oss-120b-GGUF', repoFile: 'gpt-oss-120b-mxfp4-00003-of-00003.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'GPT OSS 120b GGUF',
    description: 'GPT OSS 120b GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-1.7B-UQFF',
    modelCreator: 'EricB',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'EricB/Qwen3-1.7B-UQFF',
        fileSize: '1.49G',
        repos: [],
        files: [
          { repoName: 'EricB/Qwen3-1.7B-UQFF', repoFile: 'qwen31.7b-q4k-0.uqff' },
          {
            repoName: 'EricB/Qwen3-1.7B-UQFF',
            repoFile: 'residual.safetensors',
          },
          { repoName: 'EricB/Qwen3-1.7B-UQFF', repoFile: 'tokenizer.json' },
          { repoName: 'EricB/Qwen3-1.7B-UQFF', repoFile: 'config.json' },
          { repoName: 'EricB/Qwen3-1.7B-UQFF', repoFile: 'tokenizer_config.json' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'uqff',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'EricB/Qwen3-1.7B-UQFF summary',
    description: 'EricB/Qwen3-1.7B-UQFF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Llama-3.2-1B-Instruct-UQFF',
    modelCreator: 'EricB',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'EricB/Llama-3.2-1B-Instruct-UQFF',
        fileSize: '1.15G',
        repos: [],
        files: [
          {
            repoName: 'EricB/Llama-3.2-1B-Instruct-UQFF',
            repoFile: 'llama3.2-1b-instruct-q4k.uqff',
          },
          {
            repoName: 'EricB/Llama-3.2-1B-Instruct-UQFF',
            repoFile: 'residual.safetensors',
          },
          { repoName: 'EricB/Llama-3.2-1B-Instruct-UQFF', repoFile: 'tokenizer.json' },
          { repoName: 'EricB/Llama-3.2-1B-Instruct-UQFF', repoFile: 'config.json' },
          { repoName: 'EricB/Llama-3.2-1B-Instruct-UQFF', repoFile: 'tokenizer_config.json' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'uqff',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'EricB/Qwen3-1.7B-UQFF summary',
    description: 'EricB/Qwen3-1.7B-UQFF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-0.6B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-0.6B-GGUF',
        fileSize: '0.6G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-0.6B-GGUF', repoFile: 'Qwen3-0.6B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-0.6B-GGUF summary',
    description: 'Qwen/Qwen3-0.6B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-1.7B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-1.7B-GGUF',
        fileSize: '1.8G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-1.7B-GGUF', repoFile: 'Qwen3-1.7B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-1.7B-GGUF summary',
    description: 'Qwen/Qwen3-1.7B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-4B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-4B-GGUF',
        fileSize: '4.2G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-4B-GGUF', repoFile: 'Qwen3-4B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-4B-GGUF summary',
    description: 'Qwen/Qwen3-4B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-8B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-8B-GGUF',
        fileSize: '8.7G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-8B-GGUF', repoFile: 'Qwen3-8B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-8B-GGUF summary',
    description: 'Qwen/Qwen3-8B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-14B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-14B-GGUF',
        fileSize: '15.7G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-14B-GGUF', repoFile: 'Qwen3-14B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-14B-GGUF summary',
    description: 'Qwen/Qwen3-14B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-32B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-32B-GGUF',
        fileSize: '34.8G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-32B-GGUF', repoFile: 'Qwen3-32B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-32B-GGUF summary',
    description: 'Qwen/Qwen3-32B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-235B-A22B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-235B-A22B-GGUF',
        fileSize: '250G',
        repos: [],
        files: [
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00001-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00002-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00003-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00004-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00005-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00006-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00007-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00008-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00009-of-00009.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-235B-A22B-GGUF summary',
    description: 'Qwen/Qwen3-235B-A22B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen2-VL',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      { name: 'Qwen/Qwen2-VL-2B', fileSize: '4GB', repos: [{ repoName: 'Qwen/Qwen2-VL-2B' }], files: [] },
      {
        name: 'Qwen/Qwen2-VL-2B-Instruct',
        fileSize: '4GB',
        repos: [{ repoName: 'Qwen/Qwen2-VL-2B-Instruct' }],
        files: [],
      },
      { name: 'Qwen/Qwen2-VL-7B', fileSize: '15GB', repos: [{ repoName: 'Qwen/Qwen2-VL-7B' }], files: [] },
      {
        name: 'Qwen/Qwen2-VL-7B-Instruct',
        fileSize: '15GB',
        repos: [{ repoName: 'Qwen/Qwen2-VL-7B-Instruct' }],
        files: [],
      },
    ],
    categories: ['image-to-text'],
    backends: ['default'],
    modelType: 'vision-plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen2-VL summary',
    description: 'Qwen2-VL details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3',
    modelCreator: 'Qwen',
    modelSource: 'huggingface',
    modelOptions: [
      { name: 'Qwen/Qwen3-0.6B', fileSize: '1GB', repos: [{ repoName: 'Qwen/Qwen3-0.6B' }], files: [] },
      { name: 'Qwen/Qwen3-1.7B', fileSize: '4GB', repos: [{ repoName: 'Qwen/Qwen3-1.7B' }], files: [] },
      { name: 'Qwen/Qwen3-4B', fileSize: '8GB', repos: [{ repoName: 'Qwen/Qwen3-4B' }], files: [] },
      { name: 'Qwen/Qwen3-8B', fileSize: '16GB', repos: [{ repoName: 'Qwen/Qwen3-8B' }], files: [] },
      { name: 'Qwen/Qwen3-14B', fileSize: '28GB', repos: [{ repoName: 'Qwen/Qwen3-14B' }], files: [] },
      { name: 'Qwen/Qwen3-32B', fileSize: '64GB', repos: [{ repoName: 'Qwen/Qwen3-32B' }], files: [] },
    ],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-V3',
    modelCreator: 'deepseek-ai',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'deepseek-ai/DeepSeek-V3-Base',
        fileSize: '900GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-V3-Base' }],
        files: [],
      },
      { name: 'deepseek-ai/DeepSeek-V3', fileSize: '900GB', repos: [{ repoName: 'Qwen/DeepSeek-V3' }], files: [] },
      {
        name: 'deepseek-ai/DeepSeek-V3-0324',
        fileSize: '900GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-V3-0324' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: true,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek summary',
    description: 'DeepSeek details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1',
    modelCreator: 'deepseek-ai',
    modelSource: 'huggingface',
    modelOptions: [
      { name: 'deepseek-ai/DeepSeek-R1', fileSize: '900GB', repos: [{ repoName: 'Qwen/DeepSeek-R1' }], files: [] },
      {
        name: 'deepseek-ai/DeepSeek-R1-0528',
        fileSize: '900GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-0528' }],
        files: [],
      },
      {
        name: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',
        fileSize: '4GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B' }],
        files: [],
      },
      {
        name: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
        fileSize: '17GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B' }],
        files: [],
      },
      {
        name: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',
        fileSize: '32GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B' }],
        files: [],
      },
      {
        name: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',
        fileSize: '70GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B' }],
        files: [],
      },
      {
        name: 'deepseek-ai/DeepSeek-R1-Distill-Llama-7B',
        fileSize: '16GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-Distill-Llama-7B' }],
        files: [],
      },
      {
        name: 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',
        fileSize: '17GB',
        repos: [{ repoName: 'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: true,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek summary',
    description: 'DeepSeek details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'FLUX.1-schnell',
    modelCreator: 'black-forest-labs',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'black-forest-labs/FLUX.1-schnell',
        fileSize: '54GB',
        repos: [{ repoName: 'EricB/t5_tokenizer' }, { repoName: 'EricB/t5-v1_1-xxl-enc-only' }],
        files: [
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'flux1-schnell.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['default'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen2-VL summary',
    description: 'Qwen2-VL details',
    accessTokenRequired: true,
    extraArgs: [],
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Phi-4-multimodal-instruct',
    modelCreator: 'microsoft',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'microsoft/Phi-4-multimodal-instruct',
        fileSize: '12GB',
        repos: [{ repoName: 'microsoft/Phi-4-multimodal-instruct' }],
        files: [],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['default'],
    modelType: 'vision-plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Phi-4-multimodal-instruct summary',
    description: 'Phi-4-multimodal-instruct details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Dia-1.6B',
    modelCreator: 'nari-labs',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'nari-labs/Dia-1.6B',
        fileSize: '12.9GB',
        repos: [{ repoName: 'nari-labs/Dia-1.6B' }, { repoName: 'EricB/dac_44khz' }],
        files: [],
      },
    ],
    categories: ['text-to-speech'],
    backends: ['default'],
    modelType: 'speech',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Dia-1.6B summary',
    description: 'Dia-1.6B details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3n-E4B-it',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3n-E4B-it',
        fileSize: '12.9GB',
        repos: [{ repoName: 'google/gemma-3n-E4B-it' }],
        files: [],
      },
    ],
    categories: ['text-to-text', 'image-to-text', 'speech-to-text', 'video-to-text'],
    backends: ['default'],
    modelType: 'run',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'gemma-3n-E4B-it summary',
    description: 'gemma-3n-E4B-it details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  // {
  //   modelId: 'NousResearch-Llama-3.2-1B',
  //   modelCreator: 'NousResearch',
  //   modelSource: 'huggingface',
  //   modelOptions: [
  //     {
  //       name: 'NousResearch/Llama-3.2-1B',
  //       fileSize: '5GB',
  //       repos: [{ repoName: 'NousResearch/Llama-3.2-1B' }],
  //       files: [],
  //     },
  //   ],
  //   categories: ['text-to-text'],
  //   modelType: 'run',
  //   supportISQ: false,
  //   isAnyMoE: false,
  //   supportTool: false,
  //   supportOffloaded: false,
  //   supportThinking: false,
  //   supportMoQE: false,
  //   adapter: undefined,
  //   chatTemplate: undefined,
  //   summary: 'NousResearch-Llama-3.2-1B summary',
  //   description: 'NousResearch-Llama-3.2-1B details',
  // },
  // {
  //   modelId: 'NousResearch/Hermes-3-Llama-3.2-3B',
  //   modelCreator: 'NousResearch',
  //   modelSource: 'huggingface',
  //   modelOptions: [
  //     {
  //       name: 'NousResearch/Hermes-3-Llama-3.2-3B',
  //       fileSize: '6.5GB',
  //       repos: [{ repoName: 'NousResearch/Hermes-3-Llama-3.2-3B' }],
  //       files: [],
  //     },
  //   ],
  //   categories: ['text-to-text'],
  //   modelType: 'run',
  //   supportISQ: false,
  //   isAnyMoE: false,
  //   supportTool: false,
  //   supportOffloaded: false,
  //   supportThinking: false,
  //   supportMoQE: false,
  //   adapter: undefined,
  //   chatTemplate: undefined,
  //   summary: 'NousResearch/Hermes-3-Llama-3.2-3B summary',
  //   description: 'NousResearch/Hermes-3-Llama-3.2-3B details',
  // },
  // {
  //   modelId: 'Mistral-Nemo-Instruct-FP8-2407',
  //   modelCreator: 'mistralai',
  //   modelSource: 'huggingface',
  //   modelOptions: [
  //     {
  //       name: 'mistralai/Mistral-Nemo-Instruct-FP8-2407',
  //       fileSize: '13GB',
  //       repos: [{ repoName: 'mistralai/Mistral-Nemo-Instruct-FP8-2407' }],
  //       files: [],
  //     },
  //   ],
  //   categories: ['text-to-text'],
  //   modelType: 'run',
  //   supportISQ: false,
  //   isAnyMoE: false,
  //   supportTool: false,
  //   supportOffloaded: false,
  //   supportThinking: false,
  //   supportMoQE: false,
  //   adapter: undefined,
  //   chatTemplate: undefined,
  //   summary: 'mistralai/Mistral-Nemo-Instruct-FP8-2407 summary',
  //   description: 'mistralai/Mistral-Nemo-Instruct-FP8-2407 details',
  // },
  {
    modelId: 'FLUX.1-schnell-gguf',
    modelCreator: 'leejet',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'FLUX.1-schnell-gguf-q4_0',
        fileSize: '15.76GB',
        repos: [],
        files: [
          { repoName: 'leejet/FLUX.1-schnell-gguf', repoFile: 'flux1-schnell-q4_0.gguf' },
          { repoName: 'comfyanonymous/flux_text_encoders', repoFile: 'clip_l.safetensors' },
          { repoName: 'comfyanonymous/flux_text_encoders', repoFile: 't5xxl_fp16.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen2-VL summary',
    description: 'Qwen2-VL details',
    accessTokenRequired: true,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 4,
    defaultCfgScale: 1.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Qwen/Qwen3-0.6B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-0.6B-GGUF',
        fileSize: '0.6G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-0.6B-GGUF', repoFile: 'Qwen3-0.6B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-0.6B-GGUF summary',
    description: 'Qwen/Qwen3-0.6B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-1.7B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-1.7B-GGUF',
        fileSize: '1.8G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-1.7B-GGUF', repoFile: 'Qwen3-1.7B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-1.7B-GGUF summary',
    description: 'Qwen/Qwen3-1.7B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-4B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-4B-GGUF',
        fileSize: '4.2G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-4B-GGUF', repoFile: 'Qwen3-4B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-4B-GGUF summary',
    description: 'Qwen/Qwen3-4B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-8B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-8B-GGUF',
        fileSize: '8.7G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-8B-GGUF', repoFile: 'Qwen3-8B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-8B-GGUF summary',
    description: 'Qwen/Qwen3-8B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-14B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-14B-GGUF',
        fileSize: '15.7G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-14B-GGUF', repoFile: 'Qwen3-14B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-14B-GGUF summary',
    description: 'Qwen/Qwen3-14B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-32B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-32B-GGUF',
        fileSize: '34.8G',
        repos: [],
        files: [{ repoName: 'Qwen/Qwen3-32B-GGUF', repoFile: 'Qwen3-32B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-32B-GGUF summary',
    description: 'Qwen/Qwen3-32B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-235B-A22B-GGUF',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen/Qwen3-235B-A22B-GGUF',
        fileSize: '250G',
        repos: [],
        files: [
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00001-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00002-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00003-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00004-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00005-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00006-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00007-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00008-of-00009.gguf' },
          { repoName: 'Qwen/Qwen3-235B-A22B-GGUF', repoFile: 'Q8_0/Qwen3-235B-A22B-Q8_0-00009-of-00009.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen/Qwen3-235B-A22B-GGUF summary',
    description: 'Qwen/Qwen3-235B-A22B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Dia-1.6B',
    modelCreator: 'nari-labs',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'nari-labs/Dia-1.6B',
        fileSize: '12.9GB',
        repos: [{ repoName: 'nari-labs/Dia-1.6B' }, { repoName: 'synvek/dac_44khz' }],
        files: [],
      },
    ],
    categories: ['text-to-speech'],
    backends: ['default'],
    modelType: 'speech',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Dia-1.6B summary',
    description: 'Dia-1.6B details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'GPT-OSS-20b-GGUF',
    modelCreator: 'ggml-org',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'ggml-org/gpt-oss-20b-GGUF',
        fileSize: '11.2GB',
        repos: [],
        files: [{ repoName: 'ggml-org/gpt-oss-20b-GGUF', repoFile: 'gpt-oss-20b-mxfp4.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'GPT OSS 20b GGUF',
    description: 'GPT OSS 20b GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'GPT-OSS-120b-GGUF',
    modelCreator: 'ggml-org',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'ggml-org/gpt-oss-120b-GGUF',
        fileSize: '59GB',
        repos: [],
        files: [
          { repoName: 'ggml-org/gpt-oss-120b-GGUF', repoFile: 'gpt-oss-120b-mxfp4-00001-of-00003.gguf' },
          { repoName: 'ggml-org/gpt-oss-120b-GGUF', repoFile: 'gpt-oss-120b-mxfp4-00002-of-00003.gguf' },
          { repoName: 'ggml-org/gpt-oss-120b-GGUF', repoFile: 'gpt-oss-120b-mxfp4-00003-of-00003.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'GPT OSS 120b GGUF',
    description: 'GPT OSS 120b GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Phi-4-multimodal-instruct',
    modelCreator: 'microsoft',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'LLM-Research/Phi-4-multimodal-instruct',
        fileSize: '12GB',
        repos: [{ repoName: 'LLM-Research/Phi-4-multimodal-instruct' }],
        files: [],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['default'],
    modelType: 'vision-plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Phi-4-multimodal-instruct summary',
    description: 'Phi-4-multimodal-instruct details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'FLUX.1-schnell-gguf',
    modelCreator: 'synvek',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'FLUX.1-schnell-gguf-q4_0',
        fileSize: '16.07GB',
        repos: [],
        files: [
          { repoName: 'synvek/FLUX.1-schnell-gguf', repoFile: 'flux1-schnell-q4_0.gguf' },
          { repoName: 'synvek/flux_text_encoders', repoFile: 'clip_l.safetensors' },
          { repoName: 'synvek/flux_text_encoders', repoFile: 't5xxl_fp16.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen2-VL summary',
    description: 'Qwen2-VL details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 4,
    defaultCfgScale: 1.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'DeepSeek-R1-Distill-Llama-70B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q3_K_L-GGUF',
        fileSize: '37GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q4_K_M-GGUF',
        fileSize: '42GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q6_K-GGUF',
        fileSize: '58GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q6_K-00001-of-00002.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q6_K-00002-of-00002.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q8_0-GGUF',
        fileSize: '75GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q8_0-00001-of-00002.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q8_0-00002-of-00002.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Llama-70B-GGUF',
    description: 'DeepSeek-R1-Distill-Llama-70B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Qwen-32B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q3_K_L-GGUF',
        fileSize: '17GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M-GGUF',
        fileSize: '20GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q6_K-GGUF',
        fileSize: '27GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q8_0-GGUF',
        fileSize: '35GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Qwen-32B-GGUF',
    description: 'DeepSeek-R1-Distill-Qwen-32B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Qwen-14B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q3_K_L-GGUF',
        fileSize: '7.9GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M-GGUF',
        fileSize: '8.9GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q6_K-GGUF',
        fileSize: '12.1GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q8_0-GGUF',
        fileSize: '15.7GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Qwen-14B-GGUF',
    description: 'DeepSeek-R1-Distill-Qwen-14B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Qwen-7B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q3_K_L-GGUF',
        fileSize: '4.1GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M-GGUF',
        fileSize: '4.7GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q6_K-GGUF',
        fileSize: '6.2GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q8_0-GGUF',
        fileSize: '8.1GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Qwen-7B-GGUF',
    description: 'DeepSeek-R1-Distill-Qwen-7B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-V3-0324-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L',
        fileSize: '355GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00001-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00002-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00003-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00004-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00005-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00006-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00007-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00008-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00009-of-00009.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M-GGUF',
        fileSize: '410GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00001-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00002-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00003-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00004-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00005-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00006-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00007-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00008-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00009-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00010-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00011-of-00011.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q-GGUF',
        fileSize: '540GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00001-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00002-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00003-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00004-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00005-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00006-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00007-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00008-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00009-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00010-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00011-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00012-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00013-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00014-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00015-of-00015.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q8_0-GGUF',
        fileSize: '725GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00001-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00002-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00003-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00004-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00005-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00006-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00007-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00008-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00009-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00010-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00011-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00012-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00013-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00014-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00015-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00016-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00017-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00018-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00019-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00020-of-00020.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-V3-0324-GGUF',
    description: 'DeepSeek-V3-0324-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Q3_K_L',
        fileSize: '345GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00001-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00002-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00003-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00004-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00005-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00006-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00007-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00008-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00009-of-00009.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Q4_K_M-GGUF',
        fileSize: '398GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00001-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00002-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00003-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00004-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00005-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00006-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00007-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00008-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00009-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00010-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00011-of-00011.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Q6_Q-GGUF',
        fileSize: '520GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00001-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00002-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00003-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00004-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00005-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00006-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00007-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00008-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00009-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00010-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00011-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00012-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00013-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00014-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00015-of-00015.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Q8_0-GGUF',
        fileSize: '710GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00001-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00002-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00003-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00004-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00005-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00006-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00007-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00008-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00009-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00010-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00011-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00012-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00013-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00014-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00015-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00016-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00017-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00018-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00019-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00020-of-00020.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-GGUF',
    description: 'DeepSeek-R1-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-0528-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L',
        fileSize: '345GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00001-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00002-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00003-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00004-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00005-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00006-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00007-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00008-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00009-of-00009.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M-GGUF',
        fileSize: '398GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00001-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00002-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00003-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00004-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00005-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00006-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00007-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00008-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00009-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00010-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00011-of-00011.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q-GGUF',
        fileSize: '526GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00001-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00002-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00003-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00004-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00005-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00006-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00007-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00008-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00009-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00010-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00011-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00012-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00013-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00014-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00015-of-00015.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q8_0-GGUF',
        fileSize: '698GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00001-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00002-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00003-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00004-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00005-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00006-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00007-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00008-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00009-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00010-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00011-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00012-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00013-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00014-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00015-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00016-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00017-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00018-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00019-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00020-of-00020.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-0528-GGUF',
    description: 'DeepSeek-R1-0528-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-1b-it-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-1b-it-qat-q4_0-gguf',
        fileSize: '1G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-1b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-1b-it-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-1b-it-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-1b-pt-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-1b-pt-qat-q4_0-gguf',
        fileSize: '1G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-1b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-1b-pt-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-1b-pt-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-4b-it-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-4b-it-qat-q4_0-gguf',
        fileSize: '4G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-4b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-4b-it-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-4b-it-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-4b-pt-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-4b-pt-qat-q4_0-gguf',
        fileSize: '4G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-4b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-4b-pt-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-4b-pt-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-12b-it-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-12b-it-qat-q4_0-gguf',
        fileSize: '9G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-12b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-12b-it-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-12b-it-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-12b-pt-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-12b-pt-qat-q4_0-gguf',
        fileSize: '9G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-12b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-12b-pt-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-12b-pt-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-27b-it-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-27b-it-qat-q4_0-gguf',
        fileSize: '19G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-27b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-27b-it-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-27b-it-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-27b-pt-qat-q4_0-gguf',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/gemma-3-27b-pt-qat-q4_0-gguf',
        fileSize: '19G',
        repos: [{ repoName: 'AI-ModelScope/gemma-3-27b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'AI-ModelScope/gemma-3-27b-pt-qat-q4_0-gguf summary',
    description: 'AI-ModelScope/gemma-3-27b-pt-qat-q4_0-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Llama-70B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q3_K_L-GGUF',
        fileSize: '37GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q4_K_M-GGUF',
        fileSize: '42GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q6_K-GGUF',
        fileSize: '58GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q6_K-00001-of-00002.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q6_K-00002-of-00002.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-Q8_0-GGUF',
        fileSize: '75GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q8_0-00001-of-00002.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF', repoFile: 'DeepSeek-R1-Distill-Llama-70B-Q8_0-00002-of-00002.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Llama-70B-GGUF',
    description: 'DeepSeek-R1-Distill-Llama-70B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Qwen-32B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q3_K_L-GGUF',
        fileSize: '17GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M-GGUF',
        fileSize: '20GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q6_K-GGUF',
        fileSize: '27GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-Q8_0-GGUF',
        fileSize: '35GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Qwen-32B-GGUF',
    description: 'DeepSeek-R1-Distill-Qwen-32B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Qwen-14B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q3_K_L-GGUF',
        fileSize: '7.9GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M-GGUF',
        fileSize: '8.9GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q6_K-GGUF',
        fileSize: '12.1GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-Q8_0-GGUF',
        fileSize: '15.7GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-14B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Qwen-14B-GGUF',
    description: 'DeepSeek-R1-Distill-Qwen-14B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-Distill-Qwen-7B-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q3_K_L-GGUF',
        fileSize: '4.1GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q3_K_L.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M-GGUF',
        fileSize: '4.7GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q6_K-GGUF',
        fileSize: '6.2GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q6_K.gguf' }],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-Q8_0-GGUF',
        fileSize: '8.1GB',
        repos: [],
        files: [{ repoName: 'lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF', repoFile: 'DeepSeek-R1-Distill-Qwen-7B-Q8_0.gguf' }],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-Distill-Qwen-7B-GGUF',
    description: 'DeepSeek-R1-Distill-Qwen-7B-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-V3-0324-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L',
        fileSize: '355GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00001-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00002-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00003-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00004-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00005-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00006-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00007-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00008-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q3_K_L', repoFile: 'DeepSeek-V3-0324-Q3_K_L-00009-of-00009.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M-GGUF',
        fileSize: '410GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00001-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00002-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00003-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00004-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00005-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00006-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00007-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00008-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00009-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00010-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q4_K_M', repoFile: 'DeepSeek-V3-0324-Q4_K_M-00011-of-00011.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q-GGUF',
        fileSize: '540GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00001-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00002-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00003-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00004-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00005-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00006-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00007-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00008-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00009-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00010-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00011-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00012-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00013-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00014-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q6_Q', repoFile: 'DeepSeek-V3-0324-Q6_Q-00015-of-00015.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-V3-0324-Q8_0-GGUF',
        fileSize: '725GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00001-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00002-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00003-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00004-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00005-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00006-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00007-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00008-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00009-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00010-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00011-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00012-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00013-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00014-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00015-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00016-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00017-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00018-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00019-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-V3-0324-Q8_0', repoFile: 'DeepSeek-V3-0324-Q8_0-00020-of-00020.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-V3-0324-GGUF',
    description: 'DeepSeek-V3-0324-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-Q3_K_L',
        fileSize: '345GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00001-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00002-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00003-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00004-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00005-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00006-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00007-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00008-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q3_K_L', repoFile: 'DeepSeek-R1-Q3_K_L-00009-of-00009.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Q4_K_M-GGUF',
        fileSize: '398GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00001-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00002-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00003-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00004-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00005-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00006-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00007-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00008-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00009-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00010-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q4_K_M', repoFile: 'DeepSeek-R1-Q4_K_M-00011-of-00011.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Q6_Q-GGUF',
        fileSize: '520GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00001-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00002-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00003-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00004-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00005-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00006-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00007-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00008-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00009-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00010-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00011-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00012-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00013-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00014-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q6_Q', repoFile: 'DeepSeek-R1-Q6_Q-00015-of-00015.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-Q8_0-GGUF',
        fileSize: '710GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00001-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00002-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00003-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00004-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00005-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00006-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00007-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00008-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00009-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00010-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00011-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00012-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00013-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00014-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00015-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00016-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00017-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00018-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00019-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-Q8_0', repoFile: 'DeepSeek-R1-Q8_0-00020-of-00020.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-GGUF',
    description: 'DeepSeek-R1-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'DeepSeek-R1-0528-GGUF',
    modelCreator: 'lmstudio-community',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L',
        fileSize: '345GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00001-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00002-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00003-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00004-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00005-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00006-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00007-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00008-of-00009.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q3_K_L', repoFile: 'DeepSeek-R1-0528-Q3_K_L-00009-of-00009.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M-GGUF',
        fileSize: '398GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00001-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00002-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00003-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00004-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00005-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00006-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00007-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00008-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00009-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00010-of-00011.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q4_K_M', repoFile: 'DeepSeek-R1-0528-Q4_K_M-00011-of-00011.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q-GGUF',
        fileSize: '526GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00001-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00002-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00003-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00004-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00005-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00006-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00007-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00008-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00009-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00010-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00011-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00012-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00013-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00014-of-00015.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q6_Q', repoFile: 'DeepSeek-R1-0528-Q6_Q-00015-of-00015.gguf' },
        ],
      },
      {
        name: 'lmstudio-community/DeepSeek-R1-0528-Q8_0-GGUF',
        fileSize: '698GB',
        repos: [],
        files: [
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00001-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00002-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00003-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00004-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00005-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00006-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00007-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00008-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00009-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00010-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00011-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00012-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00013-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00014-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00015-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00016-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00017-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00018-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00019-of-00020.gguf' },
          { repoName: 'lmstudio-community/DeepSeek-R1-0528-Q8_0', repoFile: 'DeepSeek-R1-0528-Q8_0-00020-of-00020.gguf' },
        ],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'DeepSeek-R1-0528-GGUF',
    description: 'DeepSeek-R1-0528-GGUF',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-1b-it-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-1b-it-qat-q4_0-gguf',
        fileSize: '1G',
        repos: [{ repoName: 'google/gemma-3-1b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-1b-it-qat-q4_0-gguf summary',
    description: 'google/gemma-3-1b-it-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-1b-pt-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-1b-pt-qat-q4_0-gguf',
        fileSize: '1G',
        repos: [{ repoName: 'google/gemma-3-1b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-1b-pt-qat-q4_0-gguf summary',
    description: 'google/gemma-3-1b-pt-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-4b-it-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-4b-it-qat-q4_0-gguf',
        fileSize: '4G',
        repos: [{ repoName: 'google/gemma-3-4b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-4b-it-qat-q4_0-gguf summary',
    description: 'google/gemma-3-4b-it-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-4b-pt-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-4b-pt-qat-q4_0-gguf',
        fileSize: '4G',
        repos: [{ repoName: 'google/gemma-3-4b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-4b-pt-qat-q4_0-gguf summary',
    description: 'google/gemma-3-4b-pt-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-12b-it-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-12b-it-qat-q4_0-gguf',
        fileSize: '9G',
        repos: [{ repoName: 'google/gemma-3-12b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-12b-it-qat-q4_0-gguf summary',
    description: 'google/gemma-3-12b-it-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-12b-pt-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-12b-pt-qat-q4_0-gguf',
        fileSize: '9G',
        repos: [{ repoName: 'google/gemma-3-12b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-12b-pt-qat-q4_0-gguf summary',
    description: 'google/gemma-3-12b-pt-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-27b-it-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-27b-it-qat-q4_0-gguf',
        fileSize: '19G',
        repos: [{ repoName: 'google/gemma-3-27b-it-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-27b-it-qat-q4_0-gguf summary',
    description: 'google/gemma-3-27b-it-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'gemma-3-27b-pt-qat-q4_0-gguf',
    modelCreator: 'google',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'google/gemma-3-27b-pt-qat-q4_0-gguf',
        fileSize: '19G',
        repos: [{ repoName: 'google/gemma-3-27b-pt-qat-q4_0-gguf' }],
        files: [],
      },
    ],
    categories: ['text-to-text'],
    backends: ['llama_cpp', 'default'],
    modelType: 'gguf',
    supportISQ: false,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'google/gemma-3-27b-pt-qat-q4_0-gguf summary',
    description: 'google/gemma-3-27b-pt-qat-q4_0-gguf details',
    accessTokenRequired: true,
    extraArgs: [],
  },
  {
    modelId: 'stable-diffusion-3.5-large',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/stable-diffusion-3.5-large',
        fileSize: '26GB',
        repos: [],
        files: [
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large', repoFile: 'sd3.5_large.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large', repoFile: 'text_encoders/clip_g.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large', repoFile: 'text_encoders/clip_l.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large', repoFile: 'text_encoders/t5xxl_fp16.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'stable-diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'stable-diffusion-3.5-large summary',
    description: 'stable-diffusion-3.5-large details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
  },
  {
    modelId: 'stable-diffusion-3.5-medium',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/stable-diffusion-3.5-medium',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-medium', repoFile: 'sd3.5_medium.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-medium', repoFile: 'text_encoders/clip_g.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-medium', repoFile: 'text_encoders/clip_l.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-medium', repoFile: 'text_encoders/t5xxl_fp16.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'stable-diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'stable-diffusion-3.5-medium summary',
    description: 'stable-diffusion-3.5-medium details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
  },
  {
    modelId: 'stable-diffusion-3.5-large-turbo',
    modelCreator: 'AI-ModelScope',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'AI-ModelScope/stable-diffusion-3.5-large-turbo',
        fileSize: '26GB',
        repos: [],
        files: [
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large-turbo', repoFile: 'sd3.5_large_turbo.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large-turbo', repoFile: 'text_encoders/clip_g.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large-turbo', repoFile: 'text_encoders/clip_l.safetensors' },
          { repoName: 'AI-ModelScope/stable-diffusion-3.5-large-turbo', repoFile: 'text_encoders/t5xxl_fp16.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'stable-diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'stable-diffusion-3.5-large-turbo summary',
    description: 'stable-diffusion-3.5-large-turbo details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
  },
  {
    modelId: 'stable-diffusion-3.5-large',
    modelCreator: 'stabilityai',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'stabilityai/stable-diffusion-3.5-large',
        fileSize: '26GB',
        repos: [],
        files: [
          { repoName: 'stabilityai/stable-diffusion-3.5-large', repoFile: 'sd3.5_large.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-large', repoFile: 'text_encoders/clip_g.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-large', repoFile: 'text_encoders/clip_l.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-large', repoFile: 'text_encoders/t5xxl_fp16.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'stable-diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'stable-diffusion-3.5-large summary',
    description: 'stable-diffusion-3.5-large details',
    accessTokenRequired: true,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
  },
  {
    modelId: 'stable-diffusion-3.5-medium',
    modelCreator: 'stabilityai',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'stabilityai/stable-diffusion-3.5-medium',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'stabilityai/stable-diffusion-3.5-medium', repoFile: 'sd3.5_medium.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-medium', repoFile: 'text_encoders/clip_g.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-medium', repoFile: 'text_encoders/clip_l.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-medium', repoFile: 'text_encoders/t5xxl_fp16.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'stable-diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'stable-diffusion-3.5-medium summary',
    description: 'stable-diffusion-3.5-medium details',
    accessTokenRequired: true,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
  },
  {
    modelId: 'stable-diffusion-3.5-large-turbo',
    modelCreator: 'stabilityai',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'stabilityai/stable-diffusion-3.5-large-turbo',
        fileSize: '26GB',
        repos: [],
        files: [
          { repoName: 'stabilityai/stable-diffusion-3.5-large-turbo', repoFile: 'sd3.5_large_turbo.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-large-turbo', repoFile: 'text_encoders/clip_g.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-large-turbo', repoFile: 'text_encoders/clip_l.safetensors' },
          { repoName: 'stabilityai/stable-diffusion-3.5-large-turbo', repoFile: 'text_encoders/t5xxl_fp16.safetensors' },
        ],
      },
    ],
    categories: ['text-to-image'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'stable-diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: true,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'stable-diffusion-3.5-large-turbo summary',
    description: 'stable-diffusion-3.5-large-turbo details',
    accessTokenRequired: true,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
  },
  {
    modelId: 'Qwen3-0.6B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/Qwen3-0.6B', fileSize: '1.4GB', repos: [{ repoName: 'Qwen/Qwen3-0.6B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-1.7B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/Qwen3-1.7B', fileSize: '4GB', repos: [{ repoName: 'Qwen/Qwen3-1.7B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-4B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/Qwen3-4B', fileSize: '8GB', repos: [{ repoName: 'Qwen/Qwen3-4B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-8B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/Qwen3-8B', fileSize: '16GB', repos: [{ repoName: 'Qwen/Qwen3-8B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-14B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/Qwen3-14B', fileSize: '28GB', repos: [{ repoName: 'Qwen/Qwen3-14B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-32B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/Qwen3-32B', fileSize: '64GB', repos: [{ repoName: 'Qwen/Qwen3-32B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'Qwen3-235B-A22B',
    modelCreator: 'Qwen',
    modelSource: 'modelscope',
    modelOptions: [{ name: 'Qwen/-235B-A22B', fileSize: '464GB', repos: [{ repoName: 'Qwen/Qwen3-32B' }], files: [] }],
    categories: ['text-to-text'],
    backends: ['default'],
    modelType: 'plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: true,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'Qwen3 summary',
    description: 'Qwen3 details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'MiniCPM-V-4_5',
    modelCreator: 'OpenBMB',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'OpenBMB/MiniCPM-V-4_5-Q4_K_M-gguf',
        fileSize: '6GB',
        repos: [],
        files: [
          { repoName: 'OpenBMB/MiniCPM-V-4_5-gguf', repoFile: 'MiniCPM-V-4_5-Q4_K_M.gguf' },
          { repoName: 'OpenBMB/MiniCPM-V-4_5-gguf', repoFile: 'mmproj-model-f16.gguf' },
        ],
      },
      {
        name: 'OpenBMB/MiniCPM-V-4_5-Q8_0-gguf',
        fileSize: '10GB',
        repos: [],
        files: [
          { repoName: 'OpenBMB/MiniCPM-V-4_5-gguf', repoFile: 'MiniCPM-V-4_5-Q8_0.gguf' },
          { repoName: 'OpenBMB/MiniCPM-V-4_5-gguf', repoFile: 'mmproj-model-f16.gguf' },
        ],
      },
      {
        name: 'OpenBMB/MiniCPM-V-4_5-F16-gguf',
        fileSize: '17GB',
        repos: [],
        files: [
          { repoName: 'OpenBMB/MiniCPM-V-4_5-gguf', repoFile: 'MiniCPM-V-4_5-F16.gguf' },
          { repoName: 'OpenBMB/MiniCPM-V-4_5-gguf', repoFile: 'mmproj-model-f16.gguf' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['llama_cpp'],
    modelType: 'vision-plain',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'OpenBMB/MiniCPM-V-4_5-gguf summary',
    description: 'OpenBMB/MiniCPM-V-4_5-gguf details',
    accessTokenRequired: false,
    extraArgs: [],
  },
  {
    modelId: 'leejet-Ovis-Image-7B-GGUF',
    modelCreator: 'OpenBMB',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'leejet-Ovis-Image-7B-Q4_0.GGUF',
        fileSize: '10GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Ovis-Image-7B-GGUF', repoFile: 'ovis_image-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Ovis-Image', repoFile: 'split_files/text_encoders/ovis_2.5.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Ovis-Image-7B-Q8_0.GGUF',
        fileSize: '13.8GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Ovis-Image-7B-GGUF', repoFile: 'ovis_image-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Ovis-Image', repoFile: 'split_files/text_encoders/ovis_2.5.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'hf/leejet-Ovis-Image-7B-GGUF summary',
    description: 'hf/leejet-Ovis-Image-7B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'leejet-Ovis-Image-7B-GGUF',
    modelCreator: 'OpenBMB',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'leejet/Ovis-Image-7B-Q4_0.GGUF',
        fileSize: '10GB',
        repos: [],
        files: [
          { repoName: 'leejet/Ovis-Image-7B-GGUF', repoFile: 'ovis_image-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Ovis-Image', repoFile: 'split_files/text_encoders/ovis_2.5.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet/Ovis-Image-7B-Q8_0.GGUF',
        fileSize: '13.8GB',
        repos: [],
        files: [
          { repoName: 'leejet/Ovis-Image-7B-GGUF', repoFile: 'ovis_image-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Ovis-Image', repoFile: 'split_files/text_encoders/ovis_2.5.safetensors' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'leejet/Ovis-Image-7B-GGUF summary',
    description: 'leejet/Ovis-Image-7B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 5.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'leejet-Z-Image-Turbo-GGUF',
    modelCreator: 'hf',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'leejet-Z-Image-Turbo-Q8_0-GGUF',
        fileSize: '9.3GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Z-Image-Turbo-Q6_K-GGUF',
        fileSize: '7.9GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Z-Image-Turbo-Q5_0-GGUF',
        fileSize: '7.2GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Z-Image-Turbo-Q4_K-GGUF',
        fileSize: '6.6GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q4_K.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Z-Image-Turbo-Q4_0-GGUF',
        fileSize: '6.4GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Z-Image-Turbo-Q3_K-GGUF',
        fileSize: '5.8GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q3_K.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'leejet-Z-Image-Turbo-Q2_K-GGUF',
        fileSize: '5.3GB',
        repos: [],
        files: [
          { repoName: 'hf/leejet-Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q2_K.gguf.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'hf/leejet-Z-Image-Turbo-GGUF summary',
    description: 'hf/leejet-Z-Image-Turbo-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 1.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Z-Image-Turbo-GGUF',
    modelCreator: 'leejet',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Z-Image-Turbo-Q8_0-GGUF',
        fileSize: '9.3GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'Z-Image-Turbo-Q6_K-GGUF',
        fileSize: '7.9GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'Z-Image-Turbo-Q5_0-GGUF',
        fileSize: '7.2GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'Z-Image-Turbo-Q4_K-GGUF',
        fileSize: '6.6GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q4_K.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'Z-Image-Turbo-Q4_0-GGUF',
        fileSize: '6.4GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'Z-Image-Turbo-Q3_K-GGUF',
        fileSize: '5.8GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q3_K.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
      {
        name: 'Z-Image-Turbo-Q2_K-GGUF',
        fileSize: '5.3GB',
        repos: [],
        files: [
          { repoName: 'leejet/Z-Image-Turbo-GGUF', repoFile: 'z_image_turbo-Q2_K.gguf.gguf' },
          { repoName: 'unsloth/Qwen3-4B-Instruct-2507-GGUF', repoFile: 'Qwen3-4B-Instruct-2507-Q4_K_M.gguf' },
          { repoName: 'black-forest-labs/FLUX.1-schnell', repoFile: 'ae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'hf/leejet-Z-Image-Turbo-GGUF summary',
    description: 'hf/leejet-Z-Image-Turbo-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 1.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Qwen-Image-GGUF',
    modelCreator: 'QuantStack',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen_Image-Q8_0.gguf',
        fileSize: '25.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q6_K.gguf',
        fileSize: '20.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_K.gguf',
        fileSize: '17.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_K_M.gguf',
        fileSize: '18.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_1.gguf',
        fileSize: '19GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_1.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_0.gguf',
        fileSize: '18GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q4_K_S.gguf',
        fileSize: '15.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q4_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q4_K_M.gguf',
        fileSize: '17.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q4_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q4_0.gguf',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q3_K_S.gguf',
        fileSize: '12.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q3_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q3_K_M.gguf',
        fileSize: '13GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q3_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q2_K.gguf',
        fileSize: '10.95GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q2_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Qwen-Image-GGUF summary',
    description: 'QuantStack/Qwen-Image-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 2.5,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Qwen-Image-GGUF',
    modelCreator: 'QuantStack',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen_Image-Q8_0.gguf',
        fileSize: '25.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q6_K.gguf',
        fileSize: '20.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_K.gguf',
        fileSize: '17.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_K_M.gguf',
        fileSize: '18.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_1.gguf',
        fileSize: '19GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_1.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q5_0.gguf',
        fileSize: '18GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q4_K_S.gguf',
        fileSize: '15.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q4_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q4_K_M.gguf',
        fileSize: '17.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q4_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q4_0.gguf',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q3_K_S.gguf',
        fileSize: '12.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q3_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q3_K_M.gguf',
        fileSize: '13GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q3_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image-Q2_K.gguf',
        fileSize: '10.95GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-GGUF', repoFile: 'Qwen_Image-Q2_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Qwen-Image-GGUF summary',
    description: 'QuantStack/Qwen-Image-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 2.5,
  },
  {
    modelId: 'Qwen-Image-Edit-GGUF',
    modelCreator: 'QuantStack',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen_Image_Edit-Q8_0.gguf',
        fileSize: '25.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q6_K.gguf',
        fileSize: '20.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_K.gguf',
        fileSize: '17.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_K_M.gguf',
        fileSize: '18.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_1.gguf',
        fileSize: '19GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_1.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_0.gguf',
        fileSize: '18GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q4_K_S.gguf',
        fileSize: '15.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q4_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q4_K_M.gguf',
        fileSize: '17.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q4_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q4_0.gguf',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q3_K_S.gguf',
        fileSize: '12.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q3_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q3_K_M.gguf',
        fileSize: '13GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q3_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q2_K.gguf',
        fileSize: '10.95GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q2_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Qwen-Image-Edit-GGUF summary',
    description: 'QuantStack/Qwen-Image-Edit-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 2.5,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Qwen-Image-Edit-GGUF',
    modelCreator: 'QuantStack',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen_Image_Edit-Q8_0.gguf',
        fileSize: '25.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q6_K.gguf',
        fileSize: '20.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_K.gguf',
        fileSize: '17.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_K_M.gguf',
        fileSize: '18.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_1.gguf',
        fileSize: '19GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_1.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q5_0.gguf',
        fileSize: '18GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q4_K_S.gguf',
        fileSize: '15.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q4_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q4_K_M.gguf',
        fileSize: '17.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q4_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q4_0.gguf',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q3_K_S.gguf',
        fileSize: '12.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q3_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q3_K_M.gguf',
        fileSize: '13GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q3_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen_Image_Edit-Q2_K.gguf',
        fileSize: '10.95GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-GGUF', repoFile: 'Qwen_Image_Edit-Q2_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Qwen-Image-Edit-GGUF summary',
    description: 'QuantStack/Qwen-Image-Edit-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 2.5,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
  },
  {
    modelId: 'Qwen-Image-Edit-2509-GGUF',
    modelCreator: 'QuantStack',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'Qwen-Image-Edit-2509-Q8_0.gguf',
        fileSize: '25.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q6_K.gguf',
        fileSize: '20.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_K.gguf',
        fileSize: '17.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_K_M.gguf',
        fileSize: '18.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_1.gguf',
        fileSize: '19GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_1.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_0.gguf',
        fileSize: '18GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q4_K_S.gguf',
        fileSize: '15.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q4_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q4_K_M.gguf',
        fileSize: '17.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q4_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q4_0.gguf',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q3_K_S.gguf',
        fileSize: '12.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q3_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q3_K_M.gguf',
        fileSize: '13GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q3_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q2_K.gguf',
        fileSize: '10.95GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q2_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Qwen-Image-Edit-2509-GGUF summary',
    description: 'QuantStack/Qwen-Image-Edit-2509-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 2.5,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
    supportImageEdit: true,
  },
  {
    modelId: 'Qwen-Image-Edit-2509-GGUF',
    modelCreator: 'QuantStack',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'Qwen-Image-Edit-2509-Q8_0.gguf',
        fileSize: '25.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q8_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q6_K.gguf',
        fileSize: '20.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q6_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_K.gguf',
        fileSize: '17.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_K_M.gguf',
        fileSize: '18.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_1.gguf',
        fileSize: '19GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_1.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q5_0.gguf',
        fileSize: '18GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q5_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q4_K_S.gguf',
        fileSize: '15.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q4_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q4_K_M.gguf',
        fileSize: '17.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q4_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q4_0.gguf',
        fileSize: '15.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q4_0.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q3_K_S.gguf',
        fileSize: '12.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q3_K_S.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q3_K_M.gguf',
        fileSize: '13GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q3_K_M.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
      {
        name: 'Qwen-Image-Edit-2509-Q2_K.gguf',
        fileSize: '10.95GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Qwen-Image-Edit-2509-GGUF', repoFile: 'Qwen-Image-Edit-2509-Q2_K.gguf' },
          { repoName: 'unsloth/Qwen2.5-VL-7B-Instruct-GGUF', repoFile: 'Qwen2.5-VL-7B-Instruct-Q4_0.gguf' },
          { repoName: 'Comfy-Org/Qwen-Image_ComfyUI', repoFile: 'split_files/vae/qwen_image_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-text', 'image-to-text'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Qwen-Image-Edit-2509-GGUF summary',
    description: 'QuantStack/Qwen-Image-Edit-2509-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 2.5,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
    supportImageEdit: true,
  },
  {
    modelId: 'Wan2.2-TI2V-5B',
    modelCreator: 'Wan-AI',
    modelSource: 'modelscope',
    modelOptions: [
      {
        name: 'wan2.2_ti2v_5B_fp16',
        fileSize: '16.25GB',
        repos: [],
        files: [
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q8_0.gguf',
        fileSize: '12.8GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q8_0.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q6_K.gguf',
        fileSize: '11.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q6_K.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_K_S.gguf',
        fileSize: '11GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_K_S.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_K_M.gguf',
        fileSize: '11.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_K_M.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_1.gguf',
        fileSize: '11.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_1.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_0.gguf',
        fileSize: '11.1GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_0.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_K_S.gguf',
        fileSize: '10.5GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_K_S.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_K_M.gguf',
        fileSize: '10.8GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_K_M.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_1.gguf',
        fileSize: '10.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_1.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_0.gguf',
        fileSize: '10.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_0.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q3_K_S.gguf',
        fileSize: '9.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q3_K_S.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q3_K_M.gguf',
        fileSize: '9.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q3_K_M.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q2_K.gguf',
        fileSize: '9.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q2_K.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-video', 'image-to-video'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Wan2.2-TI2V-5B-GGUF summary',
    description: 'QuantStack/Wan2.2-TI2V-5B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultCfgScale: 6.0,
    defaultStepsCount: 20,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
    supportImageEdit: false,
    supportVideoGen: true,
  },
  {
    modelId: 'Wan2.2-TI2V-5B',
    modelCreator: 'Wan-AI',
    modelSource: 'huggingface',
    modelOptions: [
      {
        name: 'wan2.2_ti2v_5B_fp16',
        fileSize: '16.25GB',
        repos: [],
        files: [
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q8_0.gguf',
        fileSize: '12.8GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q8_0.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q6_K.gguf',
        fileSize: '11.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q6_K.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_K_S.gguf',
        fileSize: '11GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_K_S.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_K_M.gguf',
        fileSize: '11.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_K_M.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_1.gguf',
        fileSize: '11.3GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_1.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q5_0.gguf',
        fileSize: '11.1GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q5_0.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_K_S.gguf',
        fileSize: '10.5GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_K_S.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_K_M.gguf',
        fileSize: '10.8GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_K_M.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_1.gguf',
        fileSize: '10.6GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_1.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q4_0.gguf',
        fileSize: '10.4GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q4_0.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q3_K_S.gguf',
        fileSize: '9.7GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q3_K_S.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q3_K_M.gguf',
        fileSize: '9.9GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q3_K_M.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
      {
        name: 'Wan2.2-TI2V-5B-Q2_K.gguf',
        fileSize: '9.2GB',
        repos: [],
        files: [
          { repoName: 'QuantStack/Wan2.2-TI2V-5B-GGUF', repoFile: 'Wan2.2-TI2V-5B-Q2_K.gguf' },
          { repoName: 'city96/umt5-xxl-encoder-gguf', repoFile: 'umt5-xxl-encoder-Q8_0.gguf' },
          { repoName: 'Comfy-Org/Wan_2.2_ComfyUI_Repackaged', repoFile: 'split_files/vae/wan2.2_vae.safetensors' },
        ],
      },
    ],
    categories: ['text-to-video', 'image-to-video'],
    backends: ['stable_diffusion_cpp'],
    modelType: 'diffusion',
    supportISQ: true,
    isAnyMoE: false,
    supportTool: false,
    supportOffloaded: false,
    supportThinking: false,
    supportMoQE: false,
    adapter: undefined,
    chatTemplate: undefined,
    summary: 'QuantStack/Wan2.2-TI2V-5B-GGUF summary',
    description: 'QuantStack/Wan2.2-TI2V-5B-GGUF details',
    accessTokenRequired: false,
    extraArgs: [],
    supportStepsCount: true,
    supportCfgScale: true,
    supportNegativePrompt: true,
    defaultStepsCount: 20,
    defaultCfgScale: 6.0,
    supportOffloadedToCPU: true,
    supportDiffusionFA: true,
    supportImageEdit: false,
    supportVideoGen: true,
  },
]
